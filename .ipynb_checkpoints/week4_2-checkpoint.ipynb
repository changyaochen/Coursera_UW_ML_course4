{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# python3.6\n",
    "import json, time, sys, os\n",
    "import numpy as np                                             # dense matrices\n",
    "from scipy.sparse import csr_matrix, spdiags                   # sparse matrices\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics.pairwise import pairwise_distances        # pairwise distances\n",
    "from copy import deepcopy                                          # deep copies\n",
    "import matplotlib.pyplot as plt                                # plotting\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize                    # normalizing vectors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''compute norm of a sparse vector\n",
    "   Thanks to: Jaiyam Sharma'''\n",
    "def norm(x):\n",
    "    sum_sq = x.dot(x.T)\n",
    "    norm = np.sqrt(sum_sq)\n",
    "    return(norm)\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    \"\"\"\n",
    "    This function loads a numpy zipped file\n",
    "    and returns a sparse matrix\n",
    "    \n",
    "    csr_matrix((data, indices, indptr), [shape=(M, N)])\n",
    "        is the standard CSR representation where the column indices for row i are stored \n",
    "        in indices[indptr[i]:indptr[i+1]] and their corresponding values are stored in \n",
    "        data[indptr[i]:indptr[i+1]]. If the shape parameter is not supplied, the matrix \n",
    "        dimensions are inferred from the index arrays.\n",
    "    \n",
    "    See here for reference:\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "    \"\"\"\n",
    "    \n",
    "    loader = np.load(filename)\n",
    "    data = loader['data']\n",
    "    indices = loader['indices']\n",
    "    indptr = loader['indptr']\n",
    "    shape = loader['shape']\n",
    "    \n",
    "    return csr_matrix( (data, indices, indptr), shape)\n",
    "\n",
    "def diag(array):\n",
    "    n = len(array)\n",
    "    return spdiags(array, 0, n, n)\n",
    "\n",
    "def logpdf_diagonal_gaussian(x, mean, cov, cov_smoothing=1e-10):\n",
    "    '''\n",
    "    Compute logpdf of a multivariate Gaussian distribution with diagonal covariance at a given point x.\n",
    "    A multivariate Gaussian distribution with a diagonal covariance is equivalent\n",
    "    to a collection of independent Gaussian random variables.\n",
    "\n",
    "    x should be a sparse matrix. The logpdf will be computed for each row of x.\n",
    "    mean and cov should be given as 1D numpy arrays\n",
    "    mean[i] : mean of i-th variable\n",
    "    cov[i] : variance of i-th variable'''\n",
    "\n",
    "    n = x.shape[0]\n",
    "    dim = x.shape[1]\n",
    "    assert(dim == len(mean) and dim == len(cov))\n",
    "\n",
    "    # multiply each i-th column of x by (1/(2*sigma_i)), where sigma_i is sqrt of variance of i-th variable.\n",
    "    scaled_x = x.dot( diag(1./(2*np.sqrt(cov))) )\n",
    "    # multiply each i-th entry of mean by (1/(2*sigma_i))\n",
    "    scaled_mean = mean/(2*np.sqrt(cov))\n",
    "\n",
    "    # sum of pairwise squared Eulidean distances gives SUM[(x_i - mean_i)^2/(2*sigma_i^2)]\n",
    "    tmp = pairwise_distances(scaled_x, [scaled_mean], 'euclidean').flatten()**2 \n",
    "    return -np.sum(np.log(np.sqrt(2*np.pi*cov))) - pairwise_distances(scaled_x, [scaled_mean], 'euclidean').flatten()**2\n",
    "\n",
    "def log_sum_exp(x, axis):\n",
    "    '''Compute the log of a sum of exponentials'''\n",
    "    x_max = np.max(x, axis=axis)\n",
    "    if axis == 1:\n",
    "        return x_max + np.log( np.sum(np.exp(x-x_max[:,np.newaxis]), axis=1) )\n",
    "    else:\n",
    "        return x_max + np.log( np.sum(np.exp(x-x_max), axis=0) )\n",
    "\n",
    "def EM_for_high_dimension(data, means, covs, weights, cov_smoothing=1e-5, maxiter=int(1e3), thresh=1e-4, verbose=False):\n",
    "    # cov_smoothing: specifies the default variance assigned to absent features in a cluster.\n",
    "    #                If we were to assign zero variances to absent features, we would be overconfient,\n",
    "    #                as we hastily conclude that those featurese would NEVER appear in the cluster.\n",
    "    #                We'd like to leave a little bit of possibility for absent features to show up later.\n",
    "    n = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "    mu = deepcopy(means)\n",
    "    Sigma = deepcopy(covs)\n",
    "    K = len(mu)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    ll = None\n",
    "    ll_trace = []\n",
    "\n",
    "    for i in range(maxiter):\n",
    "        if verbose:\n",
    "            print(\"Run {} of maximum of {}\".format(i+1, maxiter))\n",
    "        # E-step: compute responsibilities\n",
    "        logresp = np.zeros((n,K))\n",
    "        for k in range(K):  # calculate (log) responsibilities, one col at a time\n",
    "            logresp[:,k] = np.log(weights[k]) + logpdf_diagonal_gaussian(data, mu[k], Sigma[k])\n",
    "        \n",
    "        ll_new = np.sum(log_sum_exp(logresp, axis=1))  # this is the \"objective function\"\n",
    "        if verbose:\n",
    "            print(ll_new)\n",
    "#             print(Sigma)\n",
    "        logresp -= np.vstack(log_sum_exp(logresp, axis=1))  # equivalent of \"N^{soft} / N\"\n",
    "        resp = np.exp(logresp)\n",
    "        counts = np.sum(resp, axis=0)  # summing along the col, to get N^{soft}\n",
    "\n",
    "        # M-step: update weights, means, covariances\n",
    "        weights = counts / np.sum(counts)\n",
    "        for k in range(K):\n",
    "            mu[k] = (diag(resp[:,k]).dot(data)).sum(axis=0)/counts[k]\n",
    "            mu[k] = mu[k].A1\n",
    "\n",
    "            Sigma[k] = diag(resp[:,k]).dot( data.multiply(data)-2*data.dot(diag(mu[k])) ).sum(axis=0) \\\n",
    "                       + (mu[k]**2)*counts[k]\n",
    "            Sigma[k] = Sigma[k].A1 / counts[k] + cov_smoothing*np.ones(dim)\n",
    "\n",
    "        \n",
    "        # check for convergence in log-likelihood\n",
    "        ll_trace.append(ll_new)\n",
    "        if ll is not None and (ll_new-ll) < thresh and ll_new > -np.inf:\n",
    "            ll = ll_new\n",
    "            break\n",
    "        else:\n",
    "            ll = ll_new\n",
    "\n",
    "    out = {'weights':weights,'means':mu,'covs':Sigma,'loglik':ll_trace,'resp':resp}\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URI</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Digby_Morrell&gt;</td>\n",
       "      <td>Digby Morrell</td>\n",
       "      <td>digby morrell born 10 october 1979 is a former...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Alfred_J._Lewy&gt;</td>\n",
       "      <td>Alfred J. Lewy</td>\n",
       "      <td>alfred j lewy aka sandy lewy graduated from un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Harpdog_Brown&gt;</td>\n",
       "      <td>Harpdog Brown</td>\n",
       "      <td>harpdog brown is a singer and harmonica player...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Franz_Rottensteiner&gt;</td>\n",
       "      <td>Franz Rottensteiner</td>\n",
       "      <td>franz rottensteiner born in waidmannsfeld lowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/G-Enka&gt;</td>\n",
       "      <td>G-Enka</td>\n",
       "      <td>henry krvits born 30 december 1974 in tallinn ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URI                 name  \\\n",
       "0        <http://dbpedia.org/resource/Digby_Morrell>        Digby Morrell   \n",
       "1       <http://dbpedia.org/resource/Alfred_J._Lewy>       Alfred J. Lewy   \n",
       "2        <http://dbpedia.org/resource/Harpdog_Brown>        Harpdog Brown   \n",
       "3  <http://dbpedia.org/resource/Franz_Rottensteiner>  Franz Rottensteiner   \n",
       "4               <http://dbpedia.org/resource/G-Enka>               G-Enka   \n",
       "\n",
       "                                                text  \n",
       "0  digby morrell born 10 october 1979 is a former...  \n",
       "1  alfred j lewy aka sandy lewy graduated from un...  \n",
       "2  harpdog brown is a singer and harmonica player...  \n",
       "3  franz rottensteiner born in waidmannsfeld lowe...  \n",
       "4  henry krvits born 30 december 1974 in tallinn ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = pd.read_csv('./people_wiki.csv', nrows=5000)\n",
    "print(wiki.shape)\n",
    "wiki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf_idf = normalize(load_sparse_csr('4_tf_idf.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conflating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>diamono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bailouts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>electionruss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>maywoods</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word\n",
       "0    conflating\n",
       "1       diamono\n",
       "2      bailouts\n",
       "3  electionruss\n",
       "4      maywoods"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./4_map_index_to_word.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "map_index_to_word = pd.DataFrame({'word': {v: k for k, v in data.items()}}).sort_index()\n",
    "\n",
    "map_index_to_word.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing mean parameters using k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "np.random.seed(5)\n",
    "num_clusters = 25\n",
    "\n",
    "# Use scikit-learn's k-means to simplify workflow\n",
    "kmeans_model = KMeans(n_clusters=num_clusters, n_init=5, max_iter=400, random_state=1, n_jobs=-1)\n",
    "kmeans_model.fit(tf_idf)\n",
    "centroids, cluster_assignment = kmeans_model.cluster_centers_, kmeans_model.labels_\n",
    "\n",
    "means = [centroid for centroid in centroids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Initializing cluster weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0512\n"
     ]
    }
   ],
   "source": [
    "num_docs = tf_idf.shape[0]\n",
    "weights = []\n",
    "for i in range(num_clusters):\n",
    "    # Compute the number of data points assigned to cluster i:\n",
    "    num_assigned = sum([x==0 for x in cluster_assignment])\n",
    "    w = float(num_assigned) / num_docs\n",
    "    weights.append(w)\n",
    "print(min(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing covariances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "covs = []\n",
    "for i in range(num_clusters):\n",
    "    member_rows = tf_idf[cluster_assignment==i]\n",
    "    cov = (member_rows.multiply(member_rows) - 2*member_rows.dot(diag(means[i]))).sum(axis=0).A1 / member_rows.shape[0] \\\n",
    "          + means[i]**2\n",
    "    cov[cov < 1e-8] = 1e-8\n",
    "    covs.append(cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 of maximum of 1000\n",
      "3855847432.46\n",
      "Run 2 of maximum of 1000\n",
      "4844053202.46\n",
      "Run 3 of maximum of 1000\n",
      "4844053202.46\n",
      "[3855847432.4638252, 4844053202.46348, 4844053202.46348]\n"
     ]
    }
   ],
   "source": [
    "out = EM_for_high_dimension(tf_idf, means, covs, weights, cov_smoothing=1e-10, verbose=True)\n",
    "print(out['loglik']) # print history of log-likelihood over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['weights', 'means', 'covs', 'loglik', 'resp'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize_EM_clusters(tf_idf, means, covs, map_index_to_word):\n",
    "    print('')\n",
    "    print('==========================================================')\n",
    "\n",
    "    num_clusters = len(means)\n",
    "    for c in range(num_clusters):\n",
    "        print('Cluster {0:d}: Largest mean parameters in cluster '.format(c))\n",
    "        print('\\n{0: <12}{1: <12}{2: <12}'.format('Word', 'Mean', 'Variance'))\n",
    "        \n",
    "        # The k'th element of sorted_word_ids should be the index of the word \n",
    "        # that has the k'th-largest value in the cluster mean. Hint: Use np.argsort().\n",
    "        sorted_word_ids = np.argsort(means[c])[::-1]\n",
    "\n",
    "        for i in sorted_word_ids[:5]:\n",
    "            print('{0: <12}{1:<10.2e}{2:10.2e}'.format(map_index_to_word['word'][i], \n",
    "                                                       means[c][i],\n",
    "                                                       covs[c][i]))\n",
    "        print('\\n=========================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================\n",
      "Cluster 0: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "minister    7.57e-02    7.42e-03\n",
      "election    5.89e-02    3.21e-03\n",
      "party       5.89e-02    2.61e-03\n",
      "liberal     2.93e-02    4.55e-03\n",
      "elected     2.91e-02    8.95e-04\n",
      "\n",
      "=========================================================\n",
      "Cluster 1: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "film        1.76e-01    6.07e-03\n",
      "films       5.50e-02    2.97e-03\n",
      "festival    4.66e-02    3.60e-03\n",
      "feature     3.69e-02    1.81e-03\n",
      "directed    3.39e-02    2.22e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 2: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "art         1.26e-01    6.83e-03\n",
      "museum      5.62e-02    7.27e-03\n",
      "gallery     3.65e-02    3.40e-03\n",
      "artist      3.61e-02    1.44e-03\n",
      "design      3.20e-02    4.59e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 3: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "basketball  1.86e-01    7.78e-03\n",
      "nba         1.01e-01    1.22e-02\n",
      "points      6.25e-02    5.92e-03\n",
      "coach       5.57e-02    5.91e-03\n",
      "team        4.68e-02    1.30e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 4: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "hockey      2.45e-01    1.64e-02\n",
      "nhl         1.56e-01    1.64e-02\n",
      "ice         6.40e-02    2.97e-03\n",
      "season      5.05e-02    2.52e-03\n",
      "league      4.31e-02    1.53e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 5: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "republican  7.93e-02    5.20e-03\n",
      "senate      5.41e-02    6.28e-03\n",
      "house       4.64e-02    2.41e-03\n",
      "district    4.60e-02    2.37e-03\n",
      "democratic  4.46e-02    3.02e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 6: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         1.60e-01    4.65e-03\n",
      "her         1.00e-01    3.14e-03\n",
      "miss        2.22e-02    7.76e-03\n",
      "women       1.43e-02    1.36e-03\n",
      "womens      1.21e-02    1.46e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 7: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "championships7.78e-02    5.17e-03\n",
      "m           4.70e-02    7.58e-03\n",
      "olympics    4.69e-02    2.59e-03\n",
      "medal       4.28e-02    2.44e-03\n",
      "she         4.18e-02    5.99e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 8: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "book        1.45e-02    9.38e-04\n",
      "published   1.23e-02    6.16e-04\n",
      "that        1.10e-02    1.73e-04\n",
      "novel       1.07e-02    1.43e-03\n",
      "he          1.04e-02    6.05e-05\n",
      "\n",
      "=========================================================\n",
      "Cluster 9: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         1.37e-01    4.25e-03\n",
      "her         8.99e-02    2.74e-03\n",
      "actress     7.65e-02    4.29e-03\n",
      "film        5.98e-02    3.44e-03\n",
      "drama       5.03e-02    6.40e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 10: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "soccer      1.15e-01    2.86e-02\n",
      "chess       4.52e-02    1.66e-02\n",
      "team        4.13e-02    2.15e-03\n",
      "coach       3.09e-02    4.45e-03\n",
      "league      3.07e-02    2.01e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 11: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "president   2.52e-02    1.29e-03\n",
      "chairman    2.44e-02    1.97e-03\n",
      "committee   2.34e-02    2.38e-03\n",
      "served      2.24e-02    6.99e-04\n",
      "executive   2.15e-02    1.23e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 12: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "music       7.26e-02    3.48e-03\n",
      "jazz        6.07e-02    1.14e-02\n",
      "hong        3.78e-02    9.92e-03\n",
      "kong        3.50e-02    8.64e-03\n",
      "chinese     3.12e-02    5.33e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 13: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "university  3.47e-02    8.89e-04\n",
      "history     3.38e-02    2.81e-03\n",
      "philosophy  2.86e-02    5.35e-03\n",
      "professor   2.74e-02    1.08e-03\n",
      "studies     2.41e-02    1.95e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 14: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "theatre     4.93e-02    6.17e-03\n",
      "actor       3.56e-02    2.91e-03\n",
      "television  3.21e-02    1.67e-03\n",
      "film        2.93e-02    1.16e-03\n",
      "comedy      2.86e-02    3.91e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 15: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "album       6.76e-02    4.78e-03\n",
      "band        5.35e-02    4.21e-03\n",
      "music       4.18e-02    1.96e-03\n",
      "released    3.13e-02    1.11e-03\n",
      "song        2.50e-02    1.81e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 16: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "tour        1.14e-01    1.92e-02\n",
      "pga         1.08e-01    2.65e-02\n",
      "racing      8.45e-02    8.26e-03\n",
      "championship6.27e-02    4.54e-03\n",
      "formula     6.06e-02    1.31e-02\n",
      "\n",
      "=========================================================\n",
      "Cluster 17: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "news        5.76e-02    8.06e-03\n",
      "radio       5.18e-02    4.62e-03\n",
      "show        3.75e-02    2.56e-03\n",
      "bbc         3.63e-02    7.41e-03\n",
      "chef        3.27e-02    1.18e-02\n",
      "\n",
      "=========================================================\n",
      "Cluster 18: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "football    1.11e-01    5.60e-03\n",
      "yards       7.37e-02    1.72e-02\n",
      "nfl         6.98e-02    9.15e-03\n",
      "coach       6.74e-02    7.85e-03\n",
      "quarterback 4.02e-02    7.16e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 19: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "league      5.21e-02    3.13e-03\n",
      "club        5.04e-02    2.64e-03\n",
      "season      4.77e-02    2.30e-03\n",
      "rugby       4.35e-02    8.18e-03\n",
      "cup         4.22e-02    2.46e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 20: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "orchestra   1.31e-01    1.06e-02\n",
      "music       1.23e-01    6.15e-03\n",
      "symphony    8.70e-02    1.08e-02\n",
      "conductor   8.16e-02    1.01e-02\n",
      "philharmonic4.96e-02    3.27e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 21: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "law         9.52e-02    8.35e-03\n",
      "court       6.84e-02    5.24e-03\n",
      "judge       4.59e-02    4.44e-03\n",
      "attorney    3.74e-02    4.30e-03\n",
      "district    3.72e-02    4.20e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 22: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "football    1.21e-01    6.14e-03\n",
      "afl         9.58e-02    1.31e-02\n",
      "australian  7.91e-02    1.58e-03\n",
      "club        5.93e-02    1.76e-03\n",
      "season      5.58e-02    1.83e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 23: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "research    5.70e-02    2.68e-03\n",
      "science     3.50e-02    2.95e-03\n",
      "university  3.34e-02    7.14e-04\n",
      "professor   3.20e-02    1.26e-03\n",
      "physics     2.61e-02    5.43e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 24: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "baseball    1.16e-01    5.57e-03\n",
      "league      1.03e-01    3.63e-03\n",
      "major       5.09e-02    1.19e-03\n",
      "games       4.66e-02    1.93e-03\n",
      "sox         4.55e-02    6.28e-03\n",
      "\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "visualize_EM_clusters(tf_idf, out['means'], out['covs'], map_index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "num_clusters = len(means)\n",
    "num_docs, num_words = tf_idf.shape\n",
    "\n",
    "random_means = []\n",
    "random_covs = []\n",
    "random_weights = []\n",
    "\n",
    "for k in range(num_clusters):\n",
    "    \n",
    "    # Create a numpy array of length num_words with random normally distributed values.\n",
    "    # Use the standard univariate normal distribution (mean 0, variance 1).\n",
    "    mean = np.random.normal(0, 1, num_words)\n",
    "    \n",
    "    # Create a numpy array of length num_words with random values uniformly distributed between 1 and 5.\n",
    "    cov = 1 + 4.*np.random.random(num_words)\n",
    "\n",
    "    # Initially give each cluster equal weight.\n",
    "    weight = 1./num_clusters\n",
    "    \n",
    "    random_means.append(mean)\n",
    "    random_covs.append(cov)\n",
    "    random_weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 of maximum of 1000\n",
      "-764085987.573\n",
      "Run 2 of maximum of 1000\n",
      "2282866699.17\n",
      "Run 3 of maximum of 1000\n",
      "2362585588.36\n",
      "Run 4 of maximum of 1000\n",
      "2362875609.17\n",
      "Run 5 of maximum of 1000\n",
      "2362875609.17\n",
      "[3855847432.4638252, 4844053202.46348, 4844053202.46348]\n"
     ]
    }
   ],
   "source": [
    "out_random_init = EM_for_high_dimension(tf_idf, random_means, random_covs, random_weights, cov_smoothing=1e-5, verbose=True)\n",
    "print(out['loglik']) # print history of log-likelihood over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================\n",
      "Cluster 0: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         4.21e-02    5.79e-03\n",
      "her         2.63e-02    1.82e-03\n",
      "music       2.03e-02    2.37e-03\n",
      "singapore   1.80e-02    5.72e-03\n",
      "bbc         1.20e-02    1.82e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 1: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "he          1.38e-02    1.11e-04\n",
      "she         1.29e-02    1.67e-03\n",
      "university  1.06e-02    3.19e-04\n",
      "music       1.05e-02    9.94e-04\n",
      "league      1.04e-02    9.58e-04\n",
      "\n",
      "=========================================================\n",
      "Cluster 2: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         3.30e-02    3.96e-03\n",
      "her         2.43e-02    2.57e-03\n",
      "music       1.46e-02    1.42e-03\n",
      "he          1.13e-02    1.20e-04\n",
      "festival    1.06e-02    2.02e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 3: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.69e-02    3.29e-03\n",
      "her         1.76e-02    1.50e-03\n",
      "film        1.43e-02    2.01e-03\n",
      "series      1.04e-02    5.20e-04\n",
      "he          1.00e-02    9.16e-05\n",
      "\n",
      "=========================================================\n",
      "Cluster 4: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.59e-02    3.38e-03\n",
      "music       1.54e-02    1.69e-03\n",
      "her         1.47e-02    1.28e-03\n",
      "he          1.24e-02    1.13e-04\n",
      "university  1.05e-02    2.92e-04\n",
      "\n",
      "=========================================================\n",
      "Cluster 5: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.73e-02    3.73e-03\n",
      "her         2.18e-02    2.47e-03\n",
      "league      2.15e-02    2.54e-03\n",
      "baseball    1.79e-02    2.28e-03\n",
      "season      1.59e-02    9.47e-04\n",
      "\n",
      "=========================================================\n",
      "Cluster 6: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         3.16e-02    3.84e-03\n",
      "her         2.92e-02    3.00e-03\n",
      "art         1.87e-02    4.07e-03\n",
      "league      1.09e-02    1.05e-03\n",
      "air         1.08e-02    3.75e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 7: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.67e-02    3.61e-03\n",
      "her         1.64e-02    1.50e-03\n",
      "university  1.27e-02    4.92e-04\n",
      "poker       1.14e-02    6.38e-03\n",
      "he          1.10e-02    1.04e-04\n",
      "\n",
      "=========================================================\n",
      "Cluster 8: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "league      2.46e-02    2.44e-03\n",
      "she         1.83e-02    2.55e-03\n",
      "season      1.82e-02    1.25e-03\n",
      "team        1.66e-02    9.01e-04\n",
      "hockey      1.65e-02    6.31e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 9: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.35e-02    2.81e-03\n",
      "her         1.70e-02    1.81e-03\n",
      "he          1.38e-02    1.23e-04\n",
      "university  1.28e-02    3.66e-04\n",
      "minister    1.00e-02    1.21e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 10: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.82e-02    3.37e-03\n",
      "her         2.11e-02    1.96e-03\n",
      "chess       1.55e-02    6.60e-03\n",
      "district    1.39e-02    1.71e-03\n",
      "university  1.33e-02    4.72e-04\n",
      "\n",
      "=========================================================\n",
      "Cluster 11: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         1.96e-02    2.49e-03\n",
      "her         1.49e-02    1.77e-03\n",
      "album       1.33e-02    1.98e-03\n",
      "music       1.30e-02    9.98e-04\n",
      "he          1.30e-02    1.11e-04\n",
      "\n",
      "=========================================================\n",
      "Cluster 12: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         3.42e-02    4.59e-03\n",
      "her         2.10e-02    1.98e-03\n",
      "film        1.59e-02    2.26e-03\n",
      "he          1.20e-02    1.15e-04\n",
      "party       1.15e-02    1.04e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 13: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         1.95e-02    3.20e-03\n",
      "he          1.27e-02    9.42e-05\n",
      "university  1.27e-02    5.70e-04\n",
      "her         1.04e-02    9.32e-04\n",
      "president   9.84e-03    3.93e-04\n",
      "\n",
      "=========================================================\n",
      "Cluster 14: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "law         2.41e-02    5.18e-03\n",
      "hong        2.03e-02    6.13e-03\n",
      "kong        1.94e-02    5.53e-03\n",
      "she         1.48e-02    2.05e-03\n",
      "band        1.44e-02    1.93e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 15: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "league      1.63e-02    1.76e-03\n",
      "she         1.49e-02    2.10e-03\n",
      "he          1.35e-02    1.08e-04\n",
      "music       1.30e-02    1.10e-03\n",
      "season      1.18e-02    8.20e-04\n",
      "\n",
      "=========================================================\n",
      "Cluster 16: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "film        2.09e-02    3.65e-03\n",
      "science     1.26e-02    2.19e-03\n",
      "research    1.20e-02    1.17e-03\n",
      "music       1.19e-02    1.26e-03\n",
      "baseball    1.18e-02    2.13e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 17: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.87e-02    3.67e-03\n",
      "orchestra   1.83e-02    4.74e-03\n",
      "symphony    1.71e-02    4.89e-03\n",
      "music       1.63e-02    1.25e-03\n",
      "her         1.54e-02    9.90e-04\n",
      "\n",
      "=========================================================\n",
      "Cluster 18: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.50e-02    4.52e-03\n",
      "music       2.05e-02    2.45e-03\n",
      "he          1.41e-02    1.33e-04\n",
      "championship1.26e-02    2.12e-03\n",
      "film        1.24e-02    1.67e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 19: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.62e-02    4.08e-03\n",
      "he          1.28e-02    1.20e-04\n",
      "her         1.22e-02    1.08e-03\n",
      "served      1.15e-02    4.49e-04\n",
      "taylor      1.13e-02    3.93e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 20: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         2.37e-02    4.14e-03\n",
      "health      1.34e-02    3.64e-03\n",
      "her         1.16e-02    9.68e-04\n",
      "he          1.15e-02    9.46e-05\n",
      "australian  1.13e-02    1.90e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 21: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "he          1.35e-02    9.19e-05\n",
      "university  1.22e-02    3.46e-04\n",
      "she         1.19e-02    1.71e-03\n",
      "football    9.61e-03    1.29e-03\n",
      "his         9.58e-03    7.85e-05\n",
      "\n",
      "=========================================================\n",
      "Cluster 22: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "church      1.71e-02    3.31e-03\n",
      "she         1.51e-02    2.41e-03\n",
      "he          1.39e-02    1.05e-04\n",
      "her         1.19e-02    1.47e-03\n",
      "music       1.10e-02    1.27e-03\n",
      "\n",
      "=========================================================\n",
      "Cluster 23: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         3.39e-02    5.30e-03\n",
      "her         2.08e-02    1.92e-03\n",
      "album       1.74e-02    3.15e-03\n",
      "music       1.52e-02    1.40e-03\n",
      "he          1.19e-02    1.22e-04\n",
      "\n",
      "=========================================================\n",
      "Cluster 24: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         1.73e-02    2.63e-03\n",
      "played      1.36e-02    6.66e-04\n",
      "football    1.31e-02    2.31e-03\n",
      "he          1.22e-02    9.16e-05\n",
      "season      1.18e-02    7.78e-04\n",
      "\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "visualize_EM_clusters(tf_idf, out_random_init['means'], out_random_init['covs'], map_index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
